pytorch的tensor与numpy的ndarray相互转化：
tensor->ndarray: `torch.tensor.numpy()` 
ndarray->tensor: `torch.from_numpy(numpy.ndarray(shape=(1,2)))`

EOF是End Of File 的缩写。

`parse()`函数与argparse模块的作用：分布式的版本是先运行一个torch的module，由那个module来启动程序，启动的时候会向它传递参数，这个参数在slurm里面看不到。

reconstruct.py文件本质就是解$|Ax-b|$，和分布式编程以及各种参数绕在一起，还涉及到Cupy的使用，看懂。

data_RL是data in real space的缩写。

```python
assert(isinstance(model, Volume))
assert(isinstance(imgs, Stack))
assert(isinstance(img, Img))

assert(isinstance(ctfs, np.ndarray))
assert(isinstance(quats, cp.ndarray))
assert(isinstance(trans, np.ndarray))
```
Volume, Stack, Img均为src代码库中的class(.py)文件。
imgs.translate/project/rotate均为kernel中的文件，用cupy编程。

本来的逻辑应该是用sorted loss梯度下降，然后output regular l2 loss。但是reconstruct现在output的也是sorted loss，故不可信。也可以直接报告分辨率FSC。

gradient error estimation（错误率估计：Dataset.py中设定错误颗粒是生成一个random数，若他大于给定错误率，则随机四元数。用bool数（0，1）设定一个flag，记录下它正确与否，看看在loss中哪些颗粒被使用了）
这里的编程逻辑有点问题：用l2 sorted loss时是在原先的loss_func里把每一个slice对应的loss排列成一个list，list.sort()之后取前百分之多少的算作loss。但是loss_func()只能访问DataLoader提供给的batch的信息，无法访问dataset，而flag又标在dataset.flags中。经询问张起，可以这样：每个线程开一个log，然后把你计算loss的步骤记录到log里面去，比如，你排序的一个batch的颗粒，排序前它们各自的编号和loss，排序后各自的编号和loss。log是python的标准库，`import logging`。

$L_{2}$ average of several points is the mean, $L_{1}$ average is the median - more rosbust to outliers.
Explaination: Assuming we have $y_{1}, y_{2}, \ldots y_{k}$ and to want get a new estimate $\beta$ based on
them. The smallest loss is obtained when we find $\beta$ which makes the derivative
of the loss to zero.
$L_1=\frac{1}{k} \sum_{i=1}^{k}\left|y_{i}-\beta\right|$
$\frac{\partial L_{1}}{\partial \beta}=-\frac{1}{k} \sum_{i=1}^{k} \operatorname{sgn}\left(y_{i}-\beta\right)$
$\operatorname{sgn}\left(y_{i}-\beta\right)$ is 1 when $y_{i}>\beta,-1$ when $y_{i}<\beta .$ The derivative equals to 0 when there is the same number of positive and negative terms among the $y_{i}-\beta,$ which means $\beta$ should be the median of $y_{i}$.
$L 2=\frac{1}{k} \sum_{i=1}^{k}\left(y_{i}-\beta\right)^{2}$
$\frac{\partial L_{2}}{\partial \beta}=-\frac{2}{k} \sum_{i=1}^{k}\left(y_{i}-\beta\right)$
$\frac{\partial L_{2}}{\partial \beta}=0 \rightarrow \beta=\frac{1}{k} \sum_{i=1}^{k} y_{i}$

Iterative Reweighted Least Squares: 
Given a set of points $\mathbf{y}_{i}$, the cost function to minimize is
$\mathbf{x}^{*}=\underset{\mathbf{x}}{\operatorname{argmin}} \sum_{i=1}^{k} d\left(\mathbf{x}, \mathbf{y}_{i}\right)$
Given a current estimate $\mathrm{x}^{t},$ the Weiszfeld algorithm
computes the next estimate $\mathrm{x}^{t+1}$ as
$\mathbf{x}^{t+1}=\frac{\sum_{i=1}^{n} w_{i}^{t} \mathbf{y}_{i}}{\sum_{i=1}^{n} w_{i}^{t}}=\operatorname{argmin}_{\mathbf{x}} \sum_{i=1}^{n} w_{i}^{t} d\left(\mathbf{x}, \mathbf{y}_{i}\right)^{2}$
where $w_{i}^{t}=1 / d\left(\mathrm{x}^{t}, \mathrm{y}_{i}\right)$
