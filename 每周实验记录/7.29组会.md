### 实验1:用同一个神经网络训练三种蛋白质
把三种蛋白质的残差混合，作为新的dataset，一起给神经网络训练。由于三种蛋白质残差大小不同，对较小的残差作zero padding（因为混合后以batch形式输入，pytorch要求batch的大小必须统一，ResNet50网络由于全局平均池化层的存在可以接受不同大小的输入）。
#### 实验结果
测试集上的average loss: 0.3258, 准确率: 87.05%。
#### 实验结果分析
可能是ResNet50识别出了三种蛋白质残差的共同特征，从而具有“可迁移性”，需要更多实验。
也可能是网络仅仅记住了三种蛋白质残差的各自的特征。

### 实验2: 用训练出的神经网络判断未做zero padding的数据
用三种蛋白质残差作zero padding的数据训练出的神经网络分别判断做/未做zero padding的数据，正确率如下。（作zero padding即模拟训练时的输入）
#### 实验结果
|  | cng | fun30 | proteasome |
| ---- |  ---- | ---- | ---- |
| zero padding | 88.13% | 83.43% | 76.75% |
| no zero padding | 57.06% | 76.58% | 76.76% |
#### 实验结果分析
用作zero padding的数据训练出的网络无法较好的判断未作zero padding的数据。

### 实验3:把网络的筛选加入重构过程
类似于之前用loss作特征，重构一遍筛一遍。首先用50%错误率的10K的数据重构出（有误差的带噪）三维结构，用此结构计算残差，zero padding后用三种蛋白质训练出的神经网络做判断。实验结果为只重构一遍筛一遍的结果。
#### 实验结果
|  | cng | fun30 | proteasome |
| ---- |  ---- | ---- | ---- |
| zero padding | 77.5% | -% | 67.5% |
(fun30用50%错误率的10K的数据重构的分辨率为18.857A，故未作实验。)
#### 回顾：sorted loss
用loss作特征，重构一遍筛一遍，反复3次。
```
loss.min() = 154.35933575455826, loss.max() = 167.88637131269212
Number of correct particles = 4980, incorrect particles = 5020
Two-sample t-statistic D = -41.54564316078004, p-value = 0.0
Number of correct particles / smaller half = 3357 / 5000

loss.min() = 194.21161434971853, loss.max() = 204.63695124643996
Number of correct particles = 4877, incorrect particles = 5123
Two-sample t-statistic D = -41.17453544268956, p-value = 0.0
Number of correct particles / smaller half = 3292 / 5000

loss.min() = 318.0670309611757, loss.max() = 321.1767782480034
Number of correct particles = 5088, incorrect particles = 4912
Two-sample t-statistic D = -121.5287611224765, p-value = 0.0
Number of correct particles / smaller half = 4146 / 5000
```

### 实验3:神经网络的可视化
问题：输入的图片（残差）肉眼无pattern。
#### 实验1: 打印特征图
输入图像经过卷积层后的特征图。
#### 实验2: Grad-CAM
用反向传播的梯度对特征图作加权，得到heatmap。
#### 结论
看不出啥。
