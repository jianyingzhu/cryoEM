Todo: 
1. 注意本机的dev新增的test 1 test 4。
1. 真值带进去计算cv loss，与估计wi的cv loss做比较。
1. 代码库里面张起的test_4.py，特别是check函数，功能是把重构结果对每个颗粒算loss，然后会输出颗粒的方向（四元数）、是否正确、loss，结果会输出到一个proteasome.log文件。然后这个文件一经产生，就不再需要每次都跑一个10K数据跑几分钟了，只要分析这个log文件就可以了。所以在我私人的仓库里面，T351/dataset.py，就是分析这个log文件的程序。文件我直接给你，你甚至不再需要访问我们的代码库了,很久没有动的cryoEM仓库,本地做数据分析即可。（proteasome.log在Downloads里）
1. 一个是去分析proteasome的那个loss histogram为啥是两个峰，是否跟优势取向有关；另一个是我提的，重构一遍刷一遍loss，找出loss比较小的部分构成新的dataset，再重构一遍，刷一遍loss，这样迭代，每轮输出resolution以及dataset的正确率等数据。然后这个东西要做的话应该需要用pytorch的Subset，参考这个：https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset
1. 集群上面的~/zhangqi/Compressed_Sensing/outpu/里面，有一个res_corr.npy和res_wrong.npy，分别是cng数据下，用ground truth对一半正确投影（其实是512个）和一半错误投影（其实是488个）给出的残差图片，就是Ax-b，第一个是512*160*160数组，第二个是488*160*160数组，可以用numpy的load函数读出来。现在问题就变成，怎么样分类这两组数据，仅仅用loss最多就分出70%的正确率出来，我试过了。相当于loss这个特征不够用，看你能不能找更多的特征。实在不行深度学习一下，看看深度学习能不能对这种东西work。
1. http://124.65.131.150:11111/T351#11840，现在我有log文件，给了true的投影角和loss，现在问，如果他说的是对的，怎么检验。怎么检验loss和投影角有显著的相关性。换句话，统计检验这个命题：loss是/不是SO(3)上的均匀分布。
散点图一画，完全没看到pattern。fun30，用loss的normalization当成颜色或者随机生成的gauss noise当颜色，投影轴的x-y当成坐标，画散点图，你能猜出来哪个是哪个么？（不能）
另外cng跟proteasome一样有优势取向，但是没有两个峰。。。


3.18组会：
1. $w_{1,i},w_{2,i}$会收敛到只有一个点为1，其他点为0：明旭说不make sense，包老师提出softmax放大了差异，类似于做了二值化，可以乘一个beta_i，或者$u_i$改为$|u_i|$。
2. 重构、筛颗粒迭代中，cng与fun30这样的图（错误与正确的分布overlap极多）才是常见的，proteasome的结果好，但其他大部分情况未必。跑一下cng, fun30，看看分辨率与重构结果。
3. 真值带进去计算cv loss，与估计wi的cv loss做比较。
4. 包老师提出，不用softmax层，直接投影。一篇论文提到，$min_x \frac{1}{2}||x-y||^2 s.t. x\geq 0, 1^Tx=1$有精确解。'Efficient Projections onto the ℓ1-Ball for Learning in High Dimensions'


- 给你安排个小任务，你去按我的test_1.py，算一下随便哪个数据集，100的颗粒（这样快一点），只跑一轮，跑一个weighted L2，然后把所有weight输出出来。注意reconstruct里面分离数据集的时候用的可能是random_split，你可以改成固定某种分离方式，比如用subset分出前50个和后50个。这样输出weight的同时也知道哪个颗粒是哪个weight。先把weight和重构结果做出来吧。
- 验证：那我对你4月1日组会所说的，cv-based weighted L2 loss 的重构结果，会比原先普通重构的 cv loss 更小。换言之，你 claim 了这样一个结论：从 half set A 里选出部分图像，重构出的模型，其投影会非常类似 half set B 里的部分图像。这个结论实在太反直觉了。

2K的颗粒，错误率为50%：
1. 用weighted L2重构，cv loss
2. 用2K的颗粒中的1K个正确颗粒做普通的L2重构，cv loss
```python
if __name__ == '__main__':
    protein = 'cng'
    choose(protein)
    dataset = Dataset(thu_path, data_path, box_size, pixel_size, random_quat_ratio = 0.5)

    '''
    # Test ground-truth residue
    corrects = np.array(dataset.corrects, dtype = int)

    with mrcfile.mmap(model_path, permissive = True, mode = 'r') as mrc:
        vol = Volume(mrc.data)
    subset = Subset(dataset, np.arange(len(dataset))[corrects]) # all correct particles
    res = residue_stack(vol, subset)
    cp.asnumpy(res.data_RL).dump(output_path + 'res_corr.npy')
    subset = Subset(dataset, np.arange(len(dataset))[~corrects])
    res = residue_stack(vol, subset)
    cp.asnumpy(res.data_RL).dump(output_path + 'res_wrong.npy')
    exit(0)
    '''

    corrects = np.array(dataset.corrects, dtype = int)
    subset = Subset(dataset, np.arange(len(dataset))[corrects]) # all correct particles
    (vol_a, vol_b) = reconstruct(subset, l2_loss_grad)
    dataset_a = Subset(subset, np.arange(len(subset) // 2))
    dataset_b = Subset(subset, np.arange(len(subset) // 2, len(subset)))

    torch_to_cupy = lambda x : cp.fromDlpack(to_dlpack(x))

    cv_loss = 0
    loader_a = DataLoader(dataset_a)
    loader_b = DataLoader(dataset_b)
    
    for (batch_a, batch_b) in zip(loader_a, loader_b):
    # for (batch_a, batch_b) in zip(loader_a, loader_b):
    # for (i, (batch_a, batch_b)) in enumerate(zip(loader_a, loader_b)):
        (bs_a, ts_a, qs_a, ctfs_a, _) = batch_a
        (bs_b, ts_b, qs_b, ctfs_b, _) = batch_b
        bs_a = Stack(data = torch_to_cupy(bs_a.to('cuda')))
        bs_b = Stack(data = torch_to_cupy(bs_b.to('cuda')))
        ts_a = ts_a.numpy()
        ts_b = ts_b.numpy()
        qs_a = torch_to_cupy(qs_a.to('cuda'))
        qs_b = torch_to_cupy(qs_b.to('cuda'))
        ctfs_a = ctfs_a.numpy()
        ctfs_b = ctfs_b.numpy()
        w_a = cp.full((len(batch_a), 1), 1. / len(batch_a), dtype = np.float64)
        w_b = cp.full((len(batch_a), 1), 1. / len(batch_b), dtype = np.float64)
        cv_loss += weighted_cv_loss(vol_a, bs_a, ts_a, qs_a, ctfs_a, w_a, vol_b, bs_b, ts_b, qs_b, ctfs_b, w_b)
    print(f'cv_loss is equal to {cv_loss}')

    # Test iterating particle filter reconstruction scheme.
    '''
    subset = dataset
    corrects = np.array(dataset.corrects, dtype = np.int8)
    for particle_num in [np.sum(corrects)]: #[6000, 6000, 5000, 5000]
        vol = do_reconstruct(subset, weighted_l2_loss_grad)
        loss = loss_list(vol, dataset, l2_loss_grad)
        idx = np.argpartition(loss, kth = particle_num)
        subset = Subset(dataset, idx[:particle_num])
        assert len(subset) == particle_num

        correct_select = np.sum(corrects[idx[:particle_num]])
        print(f'Correct particles / selected particles = {correct_select} / {particle_num} = {(correct_select / particle_num * 100):.2f}%')
    '''
```

- SP-V100、TOMO-V100、TEST、Debug-V10的优先级：Debug最高，其次TOMO，然后SP，然后TEST。小型测试（15min之内能跑完的，debug限时15min）就用Debug，其他都用SP。
- DataLoader主要是提供一个sampler。Combines a dataset and a sampler, and provides an iterable over the given dataset.
- debug的时候，如果某一行真的没有错，可以看一下上一行是不是少了一个括号。
- 代码太长的时候，把def收起来。
- DataLoader默认的batch_size = 1，会自动把一维的img补全一个纬度。能解决如下报错：
```python
File "/home/zhujianying/Compressed_Sensing/src/Stack.py", line 18, in __init__
    assert(len(data.shape) == 3)
AssertionError
```
- ENDNOTE的使用：google scholar点击cite，下载ENDNOTE格式（mac为.ris文件）->ENDNOTE中新建一个库（File->New->new a xxx Library）->双击.ris文件即导入->光标选中在.doc文件需要引用的位置，点击ENDNOTE插件->insert a citation（最左边）->选择即可（若不是尾注模式，在Preference中选择IEEE）
- dwt: discrete wavelet transform
- 功率谱：横轴：频率，纵轴：小波系数/傅立叶系数的二范数
- 得在reconstruct的过程中计算cv loss，因为w_i针对batch，需要对batch进行计算。可以让reconstruct返回一个global cv_loss：不可行。
`cv_loss += weighted_cv_loss(vol_a, bs_a, ts_a, qs_a, ctfs_a, w_a, vol_b, bs_b, ts_b, qs_b, ctfs_b, w_b)`
vol_a, vol_b需要reconstruct结束才知道，但w_a，w_b都是针对batch存储的。因此只能输出所有w_a，w_b，重构完再计算。Dataloader的batch size设置为1，这样的话w_a, w_b为一个list即可。
- `i >> j`: 位运算，对二进制数i（如果不是，则转换为二进制）向右移j位。`>>=`和`<<=`，就是对变量进行位运算移位之后的结果再赋值给原来的变量，可以类比赋值运算符+=和-=可以理解。`(i >> j) % 2` 二进制数i右移j位后取最后一位。
- `assert(tiers.shape[1] == a.ndim)`各个维度是独立的，所以指明一个tier，需要一个长度是a.ndim的数列。
- `print(np.array([[0], [5]]).shape[1]) #1`
- python中^表示按位异或，不是power。
- python运算符优先级：and > or（先计算完and再计算or）
- @property: 把类的method视作attribute。
- gpu上张起的文件夹下my_CS维护了自己的分支（gpu版本），可以在假期重构一下代码库，能加深了解。
- 
```python
import numpy as np
x = np.array([[2],[3]]) 
print(x >= 1)
print(*x.shape)
# [[ True]
#  [ True]]
# 2 1
```



