Todo: 
1. 注意本机的dev新增的test 1 test 4。
1. 真值带进去计算cv loss，与估计wi的cv loss做比较。
1. 代码库里面张起的test_4.py，特别是check函数，功能是把重构结果对每个颗粒算loss，然后会输出颗粒的方向（四元数）、是否正确、loss，结果会输出到一个proteasome.log文件。然后这个文件一经产生，就不再需要每次都跑一个10K数据跑几分钟了，只要分析这个log文件就可以了。所以在我私人的仓库里面，T351/dataset.py，就是分析这个log文件的程序。文件我直接给你，你甚至不再需要访问我们的代码库了,很久没有动的cryoEM仓库,本地做数据分析即可。（proteasome.log在Downloads里）
1. 一个是去分析proteasome的那个loss histogram为啥是两个峰，是否跟优势取向有关；另一个是我提的，重构一遍刷一遍loss，找出loss比较小的部分构成新的dataset，再重构一遍，刷一遍loss，这样迭代，每轮输出resolution以及dataset的正确率等数据。然后这个东西要做的话应该需要用pytorch的Subset，参考这个：https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset
1. 集群上面的~/zhangqi/Compressed_Sensing/outpu/里面，有一个res_corr.npy和res_wrong.npy，分别是cng数据下，用ground truth对一半正确投影（其实是512个）和一半错误投影（其实是488个）给出的残差图片，就是Ax-b，第一个是512*160*160数组，第二个是488*160*160数组，可以用numpy的load函数读出来。现在问题就变成，怎么样分类这两组数据，仅仅用loss最多就分出70%的正确率出来，我试过了。相当于loss这个特征不够用，看你能不能找更多的特征。实在不行深度学习一下，看看深度学习能不能对这种东西work。
1. http://124.65.131.150:11111/T351#11840，现在我有log文件，给了true的投影角和loss，现在问，如果他说的是对的，怎么检验。怎么检验loss和投影角有显著的相关性。换句话，统计检验这个命题：loss是/不是SO(3)上的均匀分布。
散点图一画，完全没看到pattern。fun30，用loss的normalization当成颜色或者随机生成的gauss noise当颜色，投影轴的x-y当成坐标，画散点图，你能猜出来哪个是哪个么？（不能）
另外cng跟proteasome一样有优势取向，但是没有两个峰。。。


3.18组会：
1. $w_{1,i},w_{2,i}$会收敛到只有一个点为1，其他点为0：明旭说不make sense，包老师提出softmax放大了差异，类似于做了二值化，可以乘一个beta_i，或者$u_i$改为$|u_i|$。
2. 重构、筛颗粒迭代中，cng与fun30这样的图（错误与正确的分布overlap极多）才是常见的，proteasome的结果好，但其他大部分情况未必。跑一下cng, fun30，看看分辨率与重构结果。
3. 真值带进去计算cv loss，与估计wi的cv loss做比较。
4. 包老师提出，不用softmax层，直接投影。一篇论文提到，$min_x \frac{1}{2}||x-y||^2 s.t. x\geq 0, 1^Tx=1$有精确解。'Efficient Projections onto the ℓ1-Ball for Learning in High Dimensions'


- logging.basicConfig(filename = 'home/zhujianying/Compressed_Sensing/output/test_select.log', filemode = 'w', level = logging.INFO) 不存在filename会自己生成吗？会
- Global variable is the variable created outside a function, it can be used both inside of functions and outside. But you cannot modify global variable while inside a function. You need to define it as a global variable.
```python
s = 0
def f():
    print(s)
f()
```
```
0
```
```python
s = 0
def f():
    s += 1
    print(s)
f()
```
```
UnboundLocalError: local variable 's' referenced before assignment
```
```python
s = 0
def f():
    global s
    s += 1
    print(s)
f()
```
```
1
```

- 1 epoch == 使用训练集的全部数据完成一次完整的训练。
- reconstruct中symmetrise作用在梯度grad上，使得grad满足对称性（将grad投影到例如C4空间），因为初值为0，故最终结果一定满足对称性。
- 迭代优化的一般循环：
```python
for epoch_i in range(epoch_num):
    for (batch_i, (batch_a, batch_b)) in enumerate(zip(dataloader_a, dataloader_b)):
        pass
```
```python
a = [3,4,6]
b = (1,2,3)
for i in enumerate(zip(a,b)):
    print(i)
```
```
(0, (3, 1))
(1, (4, 2))
(2, (6, 3))
```
- 显卡的存储叫显存，CPU的存储叫内存，磁盘为比如Mac air是256G。V100显存大约为30G。
- pytorch的Dataloader(pin_memory = True)，pin_memory = True使得数据读到特定的内存（pin_memory）上，因此可以直接往gpu传输，更快。（不然需要先存到pin_memory上，再从pin_memory往gpu传输）
- gpu的计算往往不耗费很多时间，瓶颈再io上，因此可以在dataloader上做文章，比如设置num_workers = 2。
- np.bool8 == np.bool_
- logging.basicConfig(filemode = 'w')代表每次覆盖原先文件并写入，另外一个选项为'a'，代表append，接在原先的内容后面。
- 你现在看到的thu都是比较好的，有的thu有一些头部信息和注释或者空行之类的东西，这个是要跳过这些玩意。跳过：当前line[0] == '#'，然后又readline了一下，当前line就没了，line就变成下一行了。
```python
# Read next line.
# Skip all empty lines and lines starting with '#'.
line = '\n'
while line != '' and (line == '\n' or line[0] == '#'):
    line = opened_file.readline()
```
- `with mrcfile.mmap(self.stack_names[i], permissive = True, mode = 'r') as mrc:`, `mrcfile.mmap(name, mode=u'r', permissive=False)`打开内存映射的文件，可加快打开大文件的速度。由于内存映射的数据阵列直接访问磁盘，因此无法使用此功能打开压缩文件。 在所有其他方式中，mmap（）的行为与open（）完全相同。
- 关于`import`：在文件夹A中有文件夹B，文件夹B中有C.py，我们需要调用C.py中的D()函数。
```python
#这种方法需要在文件夹B中建立__init__.py，__init__.py中导出D()函数：
# from .C import D
# __all__ = ['D']
import sys
sys.path.append('xx/xx/A')
from B import D
```
```python
#不导出D()函数也可以，只需从package里面再下一层，精准定位那个文件
import sys
sys.path.append('xx/xx/A')
from B.C import D
```
P.S.：import的时候会自动执行`__init__.py`，也就是`from xxx import f`会执行xxx而不止f。
理论上来说from xxx import f，它在xxx里面搜索到f这个函数就行，其实不执行xxx里的print也可？如果你import来的函数本身需要一些提前计算的话，就需要在import那个文件的时候就做掉这些计算。
- `__all__`：__all__ affects the from <module> import * behavior only. Members that are not mentioned in __all__ are still accessible from outside the module and can be imported with from <module> import <member>.
- Volume(copy = False)：跟c++的move一样，浅复制，省时间和显存。
- list的元素可以是自己定义的类，因此能把一个Volume（梯度）append到一个list上。因为python都是pass by reference，放一个reference进去而已。
- np.linalg.norm will work fine on higher-dimensional arrays.
```python
import numpy as np
x = np.random.randn(3,3,3)

print(x)
print(np.allclose(np.sqrt(np.sum(np.square(x))), np.linalg.norm(x)))    #True
```
- np.matmul == @, 与np.dot不同。
- 
```python
import logging
logging.basicConfig(filename = '/Users/jianying/Downloads/try_log.log', filemode = 'w', level = logging.INFO)   #若没有对应的文件名会新建一个log文件
logging.info('Iteration starts.')   #会自动换行
logging.info('Iteration starts.')
```
- cupy.vdot(): Returns the dot product of two vectors. The input arrays are flattened into 1-D vectors and then it performs inner product of these vectors.
- np.array(object, dtype = float64), object：An array, any object exposing the array interface, an object whose __array__ method returns an array, or any (nested) sequence. Python中的sequence：The main sequence types in Python are lists, tuples and range objects.
- cupy.asnumpy(a, stream=None, order='C'): Returns an array on the host memory from an arbitrary source array.
- 对于一般表达式来说，反斜杠后直接回车即可实现续行，使用的关键在于反斜杠后不能用空格或者其他符号。
- deviceSynchronize(): 内核执行通常是异步的，因此在GPU设备执行内核时，CPU可以继续处理其他一些命令，向设备发出更多指令，等等，而不必等待。但是，当您使用此同步命令时，会改为强制CPU空闲，直到完成所有GPU工作为止，然后再执行其他操作。
- softmax层可以令u_a, u_b的sum==0，因为只有u·的差会影响结果。
- numpy.diag(v, k=0): Extract a diagonal or construct a diagonal array. If v is a 2-D array, return a copy of its k-th diagonal. If v is a 1-D array, return a 2-D array with v on the k-th diagonal.
- Python is 与 == 区别：is 用于判断两个变量引用对象是否为同一个， == 用于判断引用变量的值是否相等。
- vol_a.bwd_FFT()： bwd == backward，为傅立叶逆变换。
- os.path.dirname(__file__)返回脚本的路径.
- 大写的是cupy里的，小写的是torch里的。
```python
torch.utils.dlpack.from_dlpack(dlpack) → Tensor
torch.utils.dlpack.to_dlpack(tensor) → PyCapsule
```
```python
cp.toDlpack()
cp.fromDlpack()
```
- np.argpartition(a, kth, axis=-1, kind='introselect', order=None):
将a中第k小的元素放在第k个位置，比他小的位于他前面，比他大的位于他后面。返回一个index(第多少小的元素的位置)，想看kth element作为pivot排序后的a，需要`a[np.argpartition(a, k)]`
kth: Element index to partition by. The k-th element will be in its final sorted position and all smaller elements will be moved before it and all larger elements behind it. 
Returns: index_arrayndarray, int
Array of indices that partition a along the specified axis. If a is one-dimensional, a[index_array] yields a partitioned a. More generally, np.take_along_axis(a, index_array, axis=a) always yields the partitioned a, irrespective of dimensionality.
- 选取前particle_num个最小的loss构成新的dataset：
```python
idx = np.argpartition(loss, kth = particle_num)
subset = Subset(dataset, idx[:particle_num])
```
- np.sum()可以对bool数组求和，只统计True元素出现的次数。
```python
import numpy as np
print(np.sum(True + True + False))
# 2
```
- 
```python
print(f'Correct particles / selected particles = {correct_select} / {particle_num} = {(correct_select / particle_num * 100):.2f}%')
```
- python文件中，函数可以调用函数体外部定义的变量值，但函数体的外部不能调用函数内部定义的变量值。若需调用，则需要在函数内部定义时声明global。
- 
```python
import numpy as np
u = np.zeros(3, dtype = np.float64)
u += 1 * [True, False, True]
print(u)
# [1. 0. 1.]
```
- 
```python
import numpy as np
u = np.array([True, False, True])
print(np.arange(3)[u])
# [0 2]
```
- 
```python
import numpy as np
u = np.array([True, False, True])
print(np.array(u, dtype = int))
print(np.arange(3)[u])
print(np.arange(3)[~u])
# [1 0 1]
# [0 2]
# [1]
```
- `exit(0)` means a clean exit without any errors / problems, while `exit(1)` means there was some issue / error / problem and that is why the program is exiting.
exit(0) == exit() and 程序return 0。return只是从一个函数返回，exit是直接从程序返回
- numpy.ndarray.dump(file): Dump a pickle of the array to the specified file. The array can be read back with pickle.load or numpy.load.
`cp.asnumpy(res.data_RL).dump(output_path + 'res_corr.npy')`
,npy: Save an array to a binary file in NumPy .npy format.
- 
```python
import numpy as np
u = np.array([True, False, True])
# [1]u.dump('/Users/jianying/Downloads/try_npy.npy')
# [2]np.save('/Users/jianying/Downloads/try_npy.npy', u)
```

