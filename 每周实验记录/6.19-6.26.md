- 本周前几天在写矩阵计算的读书报告，分别总结并实现了用Jacobi、Shift-QR、Lanczos、Arnoldi算法求矩阵特征值、矩阵奇异值分解与图像压缩。
- 针对之前的神经网络区分实际带噪声的残差的低正确率，猜测神经网络表现差的原因是因为过度拟合了背景的噪声。一个很自然的想法是去除残差的背景进行训练。
残差为$A_ix-b_i=T_iC_iP_ix-b_i$，现有的mask是针对三维model的。我们带入的x是ground truth的三维结构（不带噪声）与带噪声的真实投影图像$b_i$。问题在于mask该怎么加？只能针对三维结构加mask，而$x$是无噪声的。如果把三维的mask按照$A_i$投影，那么如果$A_i$与$b_i$不匹配，可能会因此导致$b_i$缺少一部分。可以选取mask沿着$A_i$投影后最大的半径，加圆盘Mask。
先简单的做一个实验：在带噪声的残差$Ax-b$上加一个圆盘mask，简单的取每个残差中心三分之一的圆盘，其余元素置$0$。观察针对这样的残差训练的神经网络正确率能否有些许提升。
取每个残差中心三分之一的圆盘，其余元素置$0$这一步这样实现，但运行速度很慢，3W的数据原先只需要二十分钟，现在需要一个小时以上。
```python
# delete outer 2/3 background
for i in range(m):
    for j in range(m):
        if (i - round(m / 2)) ** 2 + (j - round(m / 2)) ** 2 > m ** 2 / 36:
            res[:, i, j] = 0
```
考虑用slice对numpy ndarray整体实现。经过询问张起，可以做一个mask，然后一张残差乘一下（逐元素），一个循环就够了。这样的话用numpy实现两个同类型数组的逐元素乘比自己用for循环快，因为numpy的底层用的C/Fortran/Cuda C写的，一个用python写的，差几千倍的速度很正常。上段代码现在改为：
```python
# delete outer 2/3 background
# generate one mask
mask = np.zeros((n, n))
for i in range(n):
    for j in range(n):
        if (i - round(n / 2)) ** 2 + (j - round(n / 2)) ** 2 <= n ** 2 / 36:
            mask[i, j] = 1
for k in range(m):
    res[k, :, :] = res[k, :, :] * mask
```
结果如下:
```python
#proteasome
Defining network, loss and optimizer.
Splitting dataset into training set (80%) and testing set (20%).
Starting training.
Epoch 1, average loss of training set is 137.8651600691451, accuracy of the network on the training set: 51.11.
Epoch 1, average loss of validation set is 35.43191408597632, accuracy of the network on the validation set: 47.65.
Epoch 2, average loss of training set is 134.18880656985897, accuracy of the network on the training set: 59.54.
Epoch 2, average loss of validation set is 34.79562416038126, accuracy of the network on the validation set: 50.00.
Epoch 3, average loss of training set is 137.3113192535546, accuracy of the network on the training set: 54.01.
Epoch 3, average loss of validation set is 35.00341697620264, accuracy of the network on the validation set: 50.15.
Epoch 4, average loss of training set is 137.0450835531337, accuracy of the network on the training set: 51.02.
Epoch 4, average loss of validation set is 34.75561083151017, accuracy of the network on the validation set: 52.50.
Epoch 5, average loss of training set is 135.08054054750102, accuracy of the network on the training set: 53.26.
Epoch 5, average loss of validation set is 34.857353581567445, accuracy of the network on the validation set: 52.00.
Training completed in 5372.4689s.
#cng
Defining network, loss and optimizer.
Splitting dataset into training set (80%) and testing set (20%).
Starting training.
Epoch 1, average loss of training set is 132.93913475414564, accuracy of the network on the training set: 60.39.
Epoch 1, average loss of validation set is 35.06551278185571, accuracy of the network on the validation set: 49.60.
Epoch 2, average loss of training set is 129.1811886167839, accuracy of the network on the training set: 63.64.
Epoch 2, average loss of validation set is 35.20851607402118, accuracy of the network on the validation set: 50.00.
Epoch 3, average loss of training set is 124.29826272071585, accuracy of the network on the training set: 65.44.
Epoch 3, average loss of validation set is 36.0710546148954, accuracy of the network on the validation set: 49.25.
Epoch 4, average loss of training set is 105.74767235619962, accuracy of the network on the training set: 73.51.
Epoch 4, average loss of validation set is 39.52172523224513, accuracy of the network on the validation set: 50.35.
Epoch 5, average loss of training set is 93.3016630053313, accuracy of the network on the training set: 77.96.
Epoch 5, average loss of validation set is 43.88950427033552, accuracy of the network on the validation set: 49.80.
Training completed in 1309.7693s.
#fun30
Defining network, loss and optimizer.
Splitting dataset into training set (80%) and testing set (20%).
Starting training.
Epoch 1, average loss of training set is 133.801492465377, accuracy of the network on the training set: 62.52.
Epoch 1, average loss of validation set is 34.856100106158145, accuracy of the network on the validation set: 49.60.
Epoch 2, average loss of training set is 132.43944015951473, accuracy of the network on the training set: 59.29.
Epoch 2, average loss of validation set is 35.52328394016273, accuracy of the network on the validation set: 49.15.
Epoch 3, average loss of training set is 128.8517335761346, accuracy of the network on the training set: 61.72.
Epoch 3, average loss of validation set is 36.27963777042029, accuracy of the network on the validation set: 49.60.
Epoch 4, average loss of training set is 118.50631477803027, accuracy of the network on the training set: 68.44.
Epoch 4, average loss of validation set is 37.10604771578105, accuracy of the network on the validation set: 49.20.
Epoch 5, average loss of training set is 112.77285306095821, accuracy of the network on the training set: 69.91.
Epoch 5, average loss of validation set is 41.94139945842644, accuracy of the network on the validation set: 49.05.
Training completed in 2185.0745s.
```
可以看到正确率依然很低，哪怕在训练集上的正确率也不高（cng与fun30有缓慢提升）而验证集上正确率一直没有很大的提升。