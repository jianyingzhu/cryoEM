### 实验1:用同一个神经网络训练三种蛋白质
把三种蛋白质的残差混合，作为新的dataset，一起给神经网络训练。由于三种蛋白质残差大小不同，对较小的残差作zero padding（因为混合后以batch形式输入，pytorch要求batch的大小必须统一，ResNet50网络由于全局平均池化层的存在可以接受不同大小的输入）。
#### 实验结果
测试集上的average loss: 0.3258, 准确率: 87.05%。
#### 结论
ResNet50识别的“特征”对三种蛋白质都存在，训练好的一个网络能识别出zero padding后的三种蛋白质的错误残差。

### 实验2: 用三种蛋白质训练出一个神经网络，对三种蛋白质的残差做测试
测试时分别对三种蛋白质的残差作zero padding（模拟训练时的输入），不作zero padding，正确率如下。
#### 实验结果
|  | cng | fun30 | proteasome |
| ---- |  ---- | ---- | ---- |
| zero padding | 88.13% | 83.43% | 76.75% |
| no zero padding | 57.06% | 76.58% | 76.76% |

### 实验3:把网络的筛选加入重构过程
类似于之前用loss作特征，重构一遍筛一遍。
首先用50%错误率的10K的cng重构出三维结构，用此结构计算残差，用三种蛋白质训练出的神经网络做判断，正确率77%。

重构即用梯度下降法求解：$\mathbf{argmin_{\mathbf{x}}}\mathbf{CPx-T^{-1}b}$，其中$\mathbf{x}$为所求解的三维model。
梯度下降过程在每个batch内仅对网络判断残差为TRUE的颗粒计算平均梯度。即在每个batch内select。
#### 一个问题：是否设置下界？
如果batch内所有颗粒都被判断为错误，那会损失很多信息。目前的程序未设置下界。
#### 实验结果
以下分辨率均未进行thunder的post process。使用的ResNet50是用50%错误率的数据训练的网络。
| 颗粒数、正确率、是否神经网络筛选 | cng | fun30 | proteasome |
| ---- |  ---- | ---- | ---- |
| 10K 100% | 6.21A  | 9.43A | 4.16A |
| 10K 100% ResNet50 | 7.28A  | - | 7.48A |
| 10K 50% | 8.45A | 18.85A | 7.65A |
| 5K 100% |  7.28A |  10.56A | 4.37A |
| 10K 50% ResNet50 | 8.45A | 16.50A | 8.02A |
| 10K 30% | 7.04A  | 10.56A | 6.87A |
| 10K 30% ResNet50 | 单元格  | 单元格 | 单元格 |

#### 结论

