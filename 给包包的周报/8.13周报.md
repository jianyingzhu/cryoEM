#### 实验1：神经网络调参
超参数：
0. 网络层数：ResNet18/34/50/101/152，尝试更多网络，希望获取更简单，更方便可视化的网络。
1. epoch num: 每个epoch输出training loss
2. learning rate: 
- 固定的Learning rate
- learning rate scheduler
（1）有序调整：等间隔调整(torch.optim.lr_scheduler.StepLR())，多间隔调整(MultiStepLR())，指数衰减(ExponentialLR())，余弦退火(CosineAnnealingLR());
（2）自适应调整：依训练状况伺机而变，通过监测某个指标的变化情况(loss、accuracy)，当该指标不怎么变化时，就是调整学习率的时机(torch.optim.lr_scheduler.ReduceLROnPlateau());
（3）自定义调整：通过自定义关于epoch的lambda函数调整学习率(torch.optim.lr_scheduler.LambdaLR())。
3. batch_size: 目前设置为50，已知重构时proteasome最合适的batch_size=50，cng最合适的batch_size=20，尝试20/30/40/50/60的batch_size.
4. 输入残差统一resize的大小
5. 网络的参数net.parameters()
- 卷积层，线性参数初始化
6. etc...

#### 实验2：真实正确率为r%，拿正确的数据重构，看分辨率是否上升
##### 实验结果
需要更改！网络正在重新训练！随机性已固定！
取batch_size = 50, epoch_num = 1（该实验具有一定随机性，每次的结果有一定差异，但大小关系一致）。
|  | cng | fun30 | proteasome |
| ---- |  ---- | ---- | ---- |
| 10K 50%错误率 所有颗粒 | 8.45A | 17.6A | 7.82A |
| 10K 50%错误率 所有正确颗粒 | 7.28A | 10.56A | 4.48A |
| 10K 50%错误率 带入ground truth计算残差，用神经网络判断为正确的颗粒重构 | 7.04A | 11.00A | 6.87A |
| 10K 50%错误率 带入50%错误率的10K的颗粒重构出的三维结构计算残差，用神经网络判断为正确的颗粒重构 | 7.54A | 15.52A | 7.16A |
##### 实验结论
无论是带入ground truth计算残差（测试集上正确率为87%）再用神经网络判断为正确的颗粒重构还是带入50%错误率的10K的颗粒重构出的三维结构计算残差（重构一次筛一次正确率为70%左右）再用神经网络判断为正确的颗粒重构，都比仅仅用50%错误率的所有颗粒重构分辨率更高。但是考虑到实际数据的错误率未必高如50%，仅用70%正确率的判断方法未必能得到更好的效果。
#### 实验3：神经网络对不同蛋白质残差的可迁移性
实验：用cng训练出的网络判断proteasome, fun30的残差，其中三种蛋白质的残差都Resize成200*200（fun30的box size）。
##### 实验结果
Starting training.
Epoch: 1, data: 3980 to 4000, running_loss per pack: 0.552831.
Epoch: 1, data: 7980 to 8000, running_loss per pack: 0.416612.
Epoch: 2, data: 3980 to 4000, running_loss per pack: 0.285841.
Epoch: 2, data: 7980 to 8000, running_loss per pack: 0.307587.
Epoch: 3, data: 3980 to 4000, running_loss per pack: 0.118082.
Epoch: 3, data: 7980 to 8000, running_loss per pack: 0.211544.
Epoch: 4, data: 3980 to 4000, running_loss per pack: 0.121377.
Epoch: 4, data: 7980 to 8000, running_loss per pack: 0.143588.
Epoch: 5, data: 3980 to 4000, running_loss per pack: 0.084016.
Epoch: 5, data: 7980 to 8000, running_loss per pack: 0.107477.
Epoch: 6, data: 3980 to 4000, running_loss per pack: 0.066599.
Epoch: 6, data: 7980 to 8000, running_loss per pack: 0.072114.
Epoch: 7, data: 3980 to 4000, running_loss per pack: 0.065690.
Epoch: 7, data: 7980 to 8000, running_loss per pack: 0.072123.
Epoch: 8, data: 3980 to 4000, running_loss per pack: 0.044297.
Epoch: 8, data: 7980 to 8000, running_loss per pack: 0.074841.
Epoch: 9, data: 3980 to 4000, running_loss per pack: 0.055552.
Epoch: 9, data: 7980 to 8000, running_loss per pack: 0.069441.
Epoch: 10, data: 3980 to 4000, running_loss per pack: 0.050908.
Epoch: 10, data: 7980 to 8000, running_loss per pack: 0.049810.
Training completed in 2938.4065s.
Average loss of testing set is 0.5971392276006829, accuracy of the network on the testing set: 86.30.
Testing completed in 27.7236s.
fun30 average loss of testing set is 1.2402946816826417, accuracy of the network on the testing set: 74.88.
Testing completed in 138.3313s.
proteasome average loss of testing set is 1.469862908482898, accuracy of the network on the testing set: 69.49.
Testing completed in 143.8874s.

#### 实验4：神经网络可视化
- 针对重新训练的网络可视化。（两种方法：test_feature.py & test_grad_cam.py）
- 打印卷积核。
- target_category设置为0/1时看结果。（目前cng的10张heat map无论对错都很相似……debug）
target_category == input label：
```python
file_path = '/Users/jianying/Desktop/repositories/Compressed_Sensing/test/zhujianying'
for protein in ['cng', 'fun30', 'proteasome']:
    input_label = torch.from_numpy(np.asarray(np.load(file_path + f'/{protein}_label_10.npy')))
    print(input_label)
'''
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0])
'''
```
#### 待改进的问题：
现在的实验所有的测试(区分残差，把ground truth换成带噪重构的volume)都是对同一批数据（神经网络训练的数据），对于未知数据未做测试。

#### 调研1: Open Set Recognition

#### 调研2: Positive Unlabeled Learning

#### 调研3: 置信学习

