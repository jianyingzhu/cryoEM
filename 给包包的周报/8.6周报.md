#### 实验1：神经网络调参的超参数
0. 网络层数：ResNet18/34/50/101/152，尝试更多网络，希望获取尽可能简单的网络。
1. epoch num: 每个epoch输出training loss，validation loss。
2. learning rate: 
- 固定的Learning rate
- learning rate scheduler
（1）有序调整：等间隔调整(torch.optim.lr_scheduler.StepLR())，多间隔调整(MultiStepLR())，指数衰减(ExponentialLR())，余弦退火(CosineAnnealingLR());
（2）自适应调整：依训练状况伺机而变，通过监测某个指标的变化情况(loss、accuracy)，当该指标不怎么变化时，就是调整学习率的时机(torch.optim.lr_scheduler.ReduceLROnPlateau());
（3）自定义调整：通过自定义关于epoch的lambda函数调整学习率(torch.optim.lr_scheduler.LambdaLR())。
3. batch_size: 目前设置为50，已知重构时proteasome最合适的batch_size=50，cng最合适的batch_size=20，尝试20/30/40/50/60的batch_size.
4. 输入残差统一resize的大小: 分别尝试cng, fun30, proteasome的box size 160/200/320，250/300。
5. 网络的参数net.parameters()，调用代码如下：
- 卷积层，线性层参数初始化。
```python
from torch.nn import init
def init_weights(net, init_type='normal'):
    #print('initialization method [%s]' % init_type)
    if init_type == 'kaiming':
        net.apply(weights_init_kaiming)
    else:
        raise NotImplementedError('initialization method [%s] is not implemented' % init_type)
def weights_init_kaiming(m):
    classname = m.__class__.__name__
    #print(classname)
    if classname.find('Conv') != -1:
        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')
    elif classname.find('Linear') != -1:
        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')
    elif classname.find('BatchNorm') != -1:
        init.normal_(m.weight.data, 1.0, 0.02)
        init.constant_(m.bias.data, 0.0)
```

#### to do in next week:
1. 继续调参。
2. 在跑不同参数程序的间隙，调研: Open Set Recognition，Positive Unlabeled Learning，置信学习。
3. 组会汇报调参的结果，如果调参顺利，汇报可视化、可迁移性、真实正确率为r% 拿正确的数据重构看分辨率是否上升的结果。